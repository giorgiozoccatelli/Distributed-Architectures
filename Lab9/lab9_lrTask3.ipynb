{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, SQLTransformer\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, BooleanType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input path and constants\n",
    "inputData = \"/data/students/bigdata-01QYD/Lab9_DBD/Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Create a DataFrame from Reviews.csv\n",
    "reviews = spark.read.load(inputData,\\\n",
    "                     format=\"csv\",\\\n",
    "                     header=True,\\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProfileName: string (nullable = true)\n",
      " |-- HelpfulnessNumerator: integer (nullable = true)\n",
      " |-- HelpfulnessDenominator: integer (nullable = true)\n",
      " |-- Score: integer (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()\n",
    "#reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the records with HelpfulnessDenominator>0 (i.e., rated reviews)\n",
    "reviewsWithVotes = reviews.filter(\"HelpfulnessDenominator>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and compute the value of Column label for the selected rated reviews\n",
    "def labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator):\n",
    "    if HelpfulnessNumerator/HelpfulnessDenominator>0.9:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "spark.udf.register(\"labelAttribute\", labelAttribute, DoubleType())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class attribute\n",
    "# For this task, a review belongs to the “useful” class if its helpfulness index is above 90% (0.9).\n",
    "reviewsLabelWithVotes = reviewsWithVotes\\\n",
    ".selectExpr(\"*\", \"labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator) as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewsLabelWithVotes.printSchema()\n",
    "#reviewsLabelWithVotes.select(\"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe with Column label in training and test set\n",
    "(reviews_train, reviews_test) = reviewsLabelWithVotes.randomSplit([0.75,0.25], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Define the preprocessing steps and the classification algorithm you want to use \n",
    "# and the content of the pipeline that is used to train the model on reviews_train and apply it on reviews_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this solution we decided to use\n",
    "# - The length of text\n",
    "# - The number of words in text\n",
    "# - The length of summary\n",
    "# - The number of words in summary\n",
    "# - The number of ! appearing in text\n",
    "# - The number of ! appearing in  summary\n",
    "# - The score assigned to the reviewed item in this review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(text)>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The length of text\"\n",
    "spark.udf.register(\"lenText\", lambda text: len(text), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(text)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of words in text\"\n",
    "spark.udf.register(\"numWordsText\", lambda text: len(text.split(\" \")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(summary)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The length of summary\"\n",
    "spark.udf.register(\"lenSummary\", lambda summary: len(summary), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(summary)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of words in summary\"\n",
    "spark.udf.register(\"numWordsSummary\", lambda summary: len(summary.split(\" \")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(text)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of ! appearing in text\"\n",
    "spark.udf.register(\"numExclMark\", lambda text: len(text.split(\"!\")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(summary)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of ! appearing in summary\"\n",
    "spark.udf.register(\"numExclMarkSummary\", lambda summary: len(summary.split(\"!\")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an SQLTranformer to create the columns we are interested in\n",
    "sqlTrans = SQLTransformer(statement=\"\"\"SELECT *, \n",
    "lenText(text) AS len,\n",
    "numWordsText(text) AS numWords,\n",
    "lenSummary(summary) AS lenS,\n",
    "numWordsSummary(summary) AS numWordsS,\n",
    "numExclMark(text) AS numExclMarks,\n",
    "numExclMarkSummary(summary) AS numExclMarksS\n",
    "FROM __THIS__\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this simple solution features contain only len(Text)\n",
    "assembler = VectorAssembler(inputCols=[\"len\", \"numWords\", \"lenS\", \"numWordsS\",\n",
    "                                      \"numExclMarks\", \"numExclMarksS\",\n",
    "                                      \"Score\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model based on the logistic regression algorithm\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline that is used to create the logistic regression\n",
    "# model on the training data.\n",
    "pipeline = Pipeline().setStages([sqlTrans, assembler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/Train the model\n",
    "model = pipeline.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on the test set\n",
    "predictions = model.transform(reviews_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7229953879763735\n",
      "F1: 0.7076064281627216\n",
      "Weighted Recall: 0.7229953879763735\n",
      "Weighted Precision: 0.7167174875050183\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics\n",
    "# Accuracy, F1, weighted recall, weighted precision\n",
    "evaluatorAcc = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"accuracy\")\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"f1\")\n",
    "evaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedRecall\")\n",
    "evaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedPrecision\")\n",
    "\n",
    "print(\"Accuracy:\", evaluatorAcc.evaluate(predictions))\n",
    "print(\"F1:\", evaluatorF1.evaluate(predictions))\n",
    "print(\"Weighted Recall:\", evaluatorRecall.evaluate(predictions))\n",
    "print(\"Weighted Precision:\", evaluatorPrecision.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Predicted\n",
      "  Actual \t Useful\tUseless\n",
      "  Useful \t 41162\t\t5748\n",
      "  Useless \t 14793\t\t12451\n"
     ]
    }
   ],
   "source": [
    "#  Compute the confusion matrix\n",
    "#                     Predicted  \n",
    "#  Actual       Useful   Useless\n",
    "#  Useful          A        B\n",
    "#  Useless          C        D\n",
    "\n",
    "A = predictions.filter(\"prediction=1 and label=1\").count()\n",
    "B = predictions.filter(\"prediction=0 and label=1\").count()\n",
    "C = predictions.filter(\"prediction=1 and label=0\").count()\n",
    "D = predictions.filter(\"prediction=0 and label=0\").count()\n",
    "\n",
    "print(\"                       Predicted\")\n",
    "print(\"  Actual \\t Useful\\tUseless\")\n",
    "print(\"  Useful \\t \"+str(A)+ \"\\t\\t\"+str(B))\n",
    "print(\"  Useless \\t \"+str(C)+ \"\\t\\t\"+str(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(Useful):0.7356268429988384\n",
      "Recall(Useful):0.877467490940098\n",
      "Precision(Useless):0.6841584702456179\n",
      "Recall(Useless):0.45701805902217\n"
     ]
    }
   ],
   "source": [
    "# Precision and recall for the two classes\n",
    "# Useful\n",
    "if A+C==0:\n",
    "    print(\"Precision(Useful): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useful):\"+str(A/(A+C)))\n",
    "    \n",
    "    \n",
    "print(\"Recall(Useful):\"+str(A/(A+B)))\n",
    "\n",
    "# Useless \n",
    "if B+D==0:\n",
    "    print(\"Precision(Useless): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useless):\"+str(D/(B+D)))\n",
    "    \n",
    "print(\"Recall(Useless):\"+str(D/(C+D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
