{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, SQLTransformer\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, BooleanType, DoubleType\n",
    "from pyspark.ml.tuning import  ParamGridBuilder\n",
    "from pyspark.ml.tuning import  CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input path and constants\n",
    "inputData = \"/data/students/bigdata-01QYD/Lab9_DBD/Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Create a DataFrame from Reviews.csv\n",
    "reviews = spark.read.load(inputData,\\\n",
    "                     format=\"csv\",\\\n",
    "                     header=True,\\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProfileName: string (nullable = true)\n",
      " |-- HelpfulnessNumerator: integer (nullable = true)\n",
      " |-- HelpfulnessDenominator: integer (nullable = true)\n",
      " |-- Score: integer (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()\n",
    "#reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the records with HelpfulnessDenominator>0 (i.e., rated reviews)\n",
    "reviewsWithVotes = reviews.filter(\"HelpfulnessDenominator>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and compute the value of Column label for the selected rated reviews\n",
    "def labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator):\n",
    "    if HelpfulnessNumerator/HelpfulnessDenominator>0.9:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "spark.udf.register(\"labelAttribute\", labelAttribute, DoubleType())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class attribute\n",
    "# For this task, a review belongs to the “useful” class if its helpfulness index is above 90% (0.9).\n",
    "reviewsLabelWithVotes = reviewsWithVotes\\\n",
    ".selectExpr(\"*\", \"labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator) as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewsLabelWithVotes.printSchema()\n",
    "#reviewsLabelWithVotes.select(\"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe with Column label in training and test set\n",
    "(reviews_train, reviews_test) = reviewsLabelWithVotes.randomSplit([0.75,0.25], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Define the preprocessing steps and the classification algorithm you want to use \n",
    "# and the content of the pipeline that is used to train the model on reviews_train and apply it on reviews_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this solution we decided to use\n",
    "# - The length of text\n",
    "# - The number of words in text\n",
    "# - The length of summary\n",
    "# - The number of words in summary\n",
    "# - The number of ! appearing in text\n",
    "# - The number of ! appearing in  summary\n",
    "# - The score assigned to the reviewed item in this review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(text)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The length of text\"\n",
    "spark.udf.register(\"lenText\", lambda text: len(text), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(text)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of words in text\"\n",
    "spark.udf.register(\"numWordsText\", lambda text: len(text.split(\" \")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(summary)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The length of summary\"\n",
    "spark.udf.register(\"lenSummary\", lambda summary: len(summary), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(summary)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of words in summary\"\n",
    "spark.udf.register(\"numWordsSummary\", lambda summary: len(summary.split(\" \")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(text)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of ! appearing in text\"\n",
    "spark.udf.register(\"numExclMark\", lambda text: len(text.split(\"!\")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(summary)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the function that is used to compute \"The number of ! appearing in summary\"\n",
    "spark.udf.register(\"numExclMarkSummary\", lambda summary: len(summary.split(\"!\")), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an SQLTranformer to create the columns we are interested in\n",
    "sqlTrans = SQLTransformer(statement=\"\"\"SELECT *, \n",
    "lenText(text) AS len,\n",
    "numWordsText(text) AS numWords,\n",
    "lenSummary(summary) AS lenS,\n",
    "numWordsSummary(summary) AS numWordsS,\n",
    "numExclMark(text) AS numExclMarks,\n",
    "numExclMarkSummary(summary) AS numExclMarksS\n",
    "FROM __THIS__\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this simple solution features contain only len(Text)\n",
    "assembler = VectorAssembler(inputCols=[\"len\", \"numWords\", \"lenS\", \"numWordsS\",\n",
    "                                      \"numExclMarks\", \"numExclMarksS\",\n",
    "                                      \"Score\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model based on the decision tree algorithm\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline that is used to create the logistic regression\n",
    "# model on the training data.\n",
    "pipeline = Pipeline().setStages([sqlTrans, assembler,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a ParamGridBuilder to construct a grid of parameter values to \n",
    "# search over.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(dt.minInstancesPerNode, [1, 10])\\\n",
    ".addGrid(dt.minInfoGain, [0, 0.1])\\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now treat the Pipeline as an Estimator, wrapping it in a \n",
    "#  CrossValidator instance. This allows us to jointly choose parameters \n",
    "# for all Pipeline stages.\n",
    "# CrossValidator requires \n",
    "# - an Estimator\n",
    "# - a set of Estimator ParamMaps\n",
    "# - an Evaluator.\n",
    "cv = CrossValidator()\\\n",
    ".setEstimator(pipeline)\\\n",
    ".setEstimatorParamMaps(paramGrid)\\\n",
    ".setEvaluator(BinaryClassificationEvaluator())\\\n",
    ".setNumFolds(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/Train the model\n",
    "model = cv.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on the test set\n",
    "predictions = model.transform(reviews_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7278231787900855\n",
      "F1: 0.7195329239757465\n",
      "Weighted Recall: 0.7278231787900854\n",
      "Weighted Precision: 0.7208703237040401\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics\n",
    "# Accuracy, F1, weighted recall, weighted precision\n",
    "evaluatorAcc = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"accuracy\")\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"f1\")\n",
    "evaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedRecall\")\n",
    "evaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedPrecision\")\n",
    "\n",
    "print(\"Accuracy:\", evaluatorAcc.evaluate(predictions))\n",
    "print(\"F1:\", evaluatorF1.evaluate(predictions))\n",
    "print(\"Weighted Recall:\", evaluatorRecall.evaluate(predictions))\n",
    "print(\"Weighted Precision:\", evaluatorPrecision.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Predicted\n",
      "  Actual \t Useful\tUseless\n",
      "  Useful \t 39719\t\t7191\n",
      "  Useless \t 12992\t\t14252\n"
     ]
    }
   ],
   "source": [
    "#  Compute the confusion matrix\n",
    "#                     Predicted  \n",
    "#  Actual       Useful   Useless\n",
    "#  Useful          A        B\n",
    "#  Useless          C        D\n",
    "\n",
    "A = predictions.filter(\"prediction=1 and label=1\").count()\n",
    "B = predictions.filter(\"prediction=0 and label=1\").count()\n",
    "C = predictions.filter(\"prediction=1 and label=0\").count()\n",
    "D = predictions.filter(\"prediction=0 and label=0\").count()\n",
    "\n",
    "print(\"                       Predicted\")\n",
    "print(\"  Actual \\t Useful\\tUseless\")\n",
    "print(\"  Useful \\t \"+str(A)+ \"\\t\\t\"+str(B))\n",
    "print(\"  Useless \\t \"+str(C)+ \"\\t\\t\"+str(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(Useful):0.7535239323860294\n",
      "Recall(Useful):0.8467064591771477\n",
      "Precision(Useless):0.6646458051578604\n",
      "Recall(Useless):0.5231243576567317\n"
     ]
    }
   ],
   "source": [
    "# Precision and recall for the two classes\n",
    "# Useful\n",
    "if A+C==0:\n",
    "    print(\"Precision(Useful): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useful):\"+str(A/(A+C)))\n",
    "    \n",
    "    \n",
    "print(\"Recall(Useful):\"+str(A/(A+B)))\n",
    "\n",
    "# Useless \n",
    "if B+D==0:\n",
    "    print(\"Precision(Useless): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useless):\"+str(D/(B+D)))\n",
    "    \n",
    "print(\"Recall(Useless):\"+str(D/(C+D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
