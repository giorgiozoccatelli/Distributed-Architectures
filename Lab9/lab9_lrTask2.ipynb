{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, SQLTransformer\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, BooleanType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input path and constants\n",
    "inputData = \"/data/students/bigdata-01QYD/Lab9_DBD/Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Create a DataFrame from Reviews.csv\n",
    "reviews = spark.read.load(inputData,\\\n",
    "                     format=\"csv\",\\\n",
    "                     header=True,\\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews.printSchema()\n",
    "#reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the records with HelpfulnessDenominator>0 (i.e., rated reviews)\n",
    "reviewsWithVotes = reviews.filter(\"HelpfulnessDenominator>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and compute the value of Column label for the selected rated reviews\n",
    "def labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator):\n",
    "    if HelpfulnessNumerator/HelpfulnessDenominator>0.9:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "spark.udf.register(\"labelAttribute\", labelAttribute, DoubleType())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class attribute\n",
    "# For this task, a review belongs to the “useful” class if its helpfulness index is above 90% (0.9).\n",
    "reviewsLabelWithVotes = reviewsWithVotes\\\n",
    ".selectExpr(\"*\", \"labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator) as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewsLabelWithVotes.printSchema()\n",
    "#reviewsLabelWithVotes.select(\"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe with Column label in training and test set\n",
    "(reviews_train, reviews_test) = reviewsLabelWithVotes.randomSplit([0.75,0.25], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Define the preprocessing steps and the classification algorithm you want to use \n",
    "# and the content of the pipeline that is used to train the model on reviews_train and apply it on reviews_test\n",
    "# Implement a first solution with one single values in features: text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformer that is used to compure text length for each review\n",
    "spark.udf.register(\"lenText\", lambda text: len(text), IntegerType())\n",
    "\n",
    "# Define an SQLTranformer to create the columns we are interested in and select only the lines with \n",
    "sqlTrans = SQLTransformer(statement=\"\"\"SELECT *, \n",
    "lenText(text) AS len\n",
    "FROM __THIS__\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this simple solution features contains only len(Text)\n",
    "assembler = VectorAssembler(inputCols=[\"len\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model based on the logistic regression algorithm\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline that is used to create the logistic regression\n",
    "# model on the training data.\n",
    "pipeline = Pipeline().setStages([sqlTrans, assembler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/Train the model\n",
    "model = pipeline.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on the test set\n",
    "predictions = model.transform(reviews_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6326024219866764\n",
      "F1: 0.4902428404049922\n",
      "Weighted Recall: 0.6326024219866764\n",
      "Weighted Precision: 0.40018582430340893\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics\n",
    "# Accuracy, F1, weighted recall, weighted precision\n",
    "evaluatorAcc = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"accuracy\")\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"f1\")\n",
    "evaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedRecall\")\n",
    "evaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedPrecision\")\n",
    "\n",
    "print(\"Accuracy:\", evaluatorAcc.evaluate(predictions))\n",
    "print(\"F1:\", evaluatorF1.evaluate(predictions))\n",
    "print(\"Weighted Recall:\", evaluatorRecall.evaluate(predictions))\n",
    "print(\"Weighted Precision:\", evaluatorPrecision.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Predicted\n",
      "  Actual \t Useful\tUseless\n",
      "  Useful \t 46910\t\t0\n",
      "  Useless \t 27244\t\t0\n"
     ]
    }
   ],
   "source": [
    "#  Compute the confusion matrix\n",
    "#                     Predicted  \n",
    "#  Actual       Useful   Useless\n",
    "#  Useful          A        B\n",
    "#  Useless          C        D\n",
    "\n",
    "A = predictions.filter(\"prediction=1 and label=1\").count()\n",
    "B = predictions.filter(\"prediction=0 and label=1\").count()\n",
    "C = predictions.filter(\"prediction=1 and label=0\").count()\n",
    "D = predictions.filter(\"prediction=0 and label=0\").count()\n",
    "\n",
    "print(\"                       Predicted\")\n",
    "print(\"  Actual \\t Useful\\tUseless\")\n",
    "print(\"  Useful \\t \"+str(A)+ \"\\t\\t\"+str(B))\n",
    "print(\"  Useless \\t \"+str(C)+ \"\\t\\t\"+str(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(Useful):0.6326024219866764\n",
      "Recall(Useful):1.0\n",
      "Precision(Useless): undefined\n",
      "Recall(Useless):0.0\n"
     ]
    }
   ],
   "source": [
    "# Precision and recall for the two classes\n",
    "# Useful\n",
    "if A+C==0:\n",
    "    print(\"Precision(Useful): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useful):\"+str(A/(A+C)))\n",
    "    \n",
    "    \n",
    "print(\"Recall(Useful):\"+str(A/(A+B)))\n",
    "\n",
    "# Useless \n",
    "if B+D==0:\n",
    "    print(\"Precision(Useless): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useless):\"+str(D/(B+D)))\n",
    "    \n",
    "print(\"Recall(Useless):\"+str(D/(C+D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
