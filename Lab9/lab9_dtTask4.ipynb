{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, IndexToString, SQLTransformer\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType, BooleanType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input path and constants\n",
    "inputData = \"/data/students/bigdata-01QYD/Lab9_DBD/Reviews.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Create a DataFrame from Reviews.csv\n",
    "reviews = spark.read.load(inputData,\\\n",
    "                     format=\"csv\",\\\n",
    "                     header=True,\\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProfileName: string (nullable = true)\n",
      " |-- HelpfulnessNumerator: integer (nullable = true)\n",
      " |-- HelpfulnessDenominator: integer (nullable = true)\n",
      " |-- Score: integer (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()\n",
    "#reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the records with HelpfulnessDenominator>0 (i.e., rated reviews)\n",
    "reviewsWithVotes = reviews.filter(\"HelpfulnessDenominator>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and compute the value of Column label for the selected rated reviews\n",
    "def labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator):\n",
    "    if HelpfulnessNumerator/HelpfulnessDenominator>0.9:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "spark.udf.register(\"labelAttribute\", labelAttribute, DoubleType())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the class attribute\n",
    "# For this task, a review belongs to the “useful” class if its helpfulness index is above 90% (0.9).\n",
    "reviewsLabelWithVotes = reviewsWithVotes\\\n",
    ".selectExpr(\"*\", \"labelAttribute(HelpfulnessNumerator, HelpfulnessDenominator) as label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviewsLabelWithVotes.printSchema()\n",
    "#reviewsLabelWithVotes.select(\"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe with Column label in training and test set\n",
    "(reviews_train, reviews_test) = reviewsLabelWithVotes.randomSplit([0.75,0.25], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Define the preprocessing steps and the classification algorithm you want to use \n",
    "# and the content of the pipeline that is used to train the model on reviews_train and apply it on reviews_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this solution we decided to use the text field\n",
    "# Configure a preprocessing phase fo the text field that consists of the following stages: \n",
    "# tokenizer -> split sentences in set of words\n",
    "# remover -> remove stopwords\n",
    "# hashingTF -> map set of words to a fixed-length feature vectors  (each \n",
    "# word becomes a feature and the value of the feature is the frequency of\n",
    "#  the word in the sentence)\n",
    "# idf -> compute the idf component of the TF-IDF measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tokenizer splits each sentence in a set of words.\n",
    "# It analyzes the content of column \"text\" and adds the \n",
    "# new column \"words\" in the returned DataFrame\n",
    "tokenizer = Tokenizer().setInputCol(\"Text\").setOutputCol(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords.\n",
    "# The StopWordsRemover component returns a new DataFrame with \n",
    "# a new column called \"filteredWords\". \"filteredWords\" is generated \n",
    "# by removing the stopwords from the content of column \"words\" \n",
    "#remover = StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filteredWords\")\n",
    "remover = StopWordsRemover()\\\n",
    ".setInputCol(\"words\")\\\n",
    ".setOutputCol(\"filteredWords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to a features\n",
    "# Each word in filteredWords must become a feature in a Vector object\n",
    "# The HashingTF Transformer can be used to perform this operation.\n",
    "# This operations is based on a hash function and can potentially \n",
    "# map two different words to the same \"feature\". The number of conflicts\n",
    "# in influenced by the value of the numFeatures parameter.  \n",
    "# The \"feature\" version of the words is stored in Column \"rawFeatures\". \n",
    "# Each feature, for a document, contains the number of occurrences \n",
    "# of that feature in the document (TF component of the TF-IDF measure) \n",
    "hashingTF = HashingTF()\\\n",
    ".setNumFeatures(1000)\\\n",
    ".setInputCol(\"filteredWords\")\\\n",
    ".setOutputCol(\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the IDF transformation/computation.\n",
    "# Update the weight associated with each feature by considering also the \n",
    "# inverse document frequency component. The returned new column \n",
    "# is called \"features\", that is the standard name for the column that \n",
    "# contains the  predictive features used to create a classification model \n",
    "idf = IDF()\\\n",
    ".setInputCol(\"rawFeatures\")\\\n",
    ".setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model based on the decision tree algorithm\n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline that is used to create the logistic regression\n",
    "# model on the training data.\n",
    "pipeline = Pipeline().setStages([tokenizer, remover, hashingTF, idf,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/Train the model\n",
    "model = pipeline.fit(reviews_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model on the test set\n",
    "predictions = model.transform(reviews_test).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6374738264099966\n",
      "F1: 0.5002284137449373\n",
      "Weighted Recall: 0.6374738264099966\n",
      "Weighted Precision: 0.6563767477929765\n"
     ]
    }
   ],
   "source": [
    "# Compute statistics\n",
    "# Accuracy, F1, weighted recall, weighted precision\n",
    "evaluatorAcc = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"accuracy\")\n",
    "evaluatorF1 = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"f1\")\n",
    "evaluatorRecall = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedRecall\")\n",
    "evaluatorPrecision = MulticlassClassificationEvaluator(labelCol=\"label\" , predictionCol= \"prediction\", metricName = \"weightedPrecision\")\n",
    "\n",
    "print(\"Accuracy:\", evaluatorAcc.evaluate(predictions))\n",
    "print(\"F1:\", evaluatorF1.evaluate(predictions))\n",
    "print(\"Weighted Recall:\", evaluatorRecall.evaluate(predictions))\n",
    "print(\"Weighted Precision:\", evaluatorPrecision.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Predicted\n",
      "  Actual \t Useful\tUseless\n",
      "  Useful \t 46980\t\t94\n",
      "  Useless \t 26742\t\t209\n"
     ]
    }
   ],
   "source": [
    "#  Compute the confusion matrix\n",
    "#                     Predicted  \n",
    "#  Actual       Useful   Useless\n",
    "#  Useful          A        B\n",
    "#  Useless          C        D\n",
    "\n",
    "A = predictions.filter(\"prediction=1 and label=1\").count()\n",
    "B = predictions.filter(\"prediction=0 and label=1\").count()\n",
    "C = predictions.filter(\"prediction=1 and label=0\").count()\n",
    "D = predictions.filter(\"prediction=0 and label=0\").count()\n",
    "\n",
    "print(\"                       Predicted\")\n",
    "print(\"  Actual \\t Useful\\tUseless\")\n",
    "print(\"  Useful \\t \"+str(A)+ \"\\t\\t\"+str(B))\n",
    "print(\"  Useless \\t \"+str(C)+ \"\\t\\t\"+str(D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(Useful):0.6372588915113535\n",
      "Recall(Useful):0.9980031439860645\n",
      "Precision(Useless):0.6897689768976898\n",
      "Recall(Useless):0.007754814292605098\n"
     ]
    }
   ],
   "source": [
    "# Precision and recall for the two classes\n",
    "# Useful\n",
    "if A+C==0:\n",
    "    print(\"Precision(Useful): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useful):\"+str(A/(A+C)))\n",
    "    \n",
    "    \n",
    "print(\"Recall(Useful):\"+str(A/(A+B)))\n",
    "\n",
    "# Useless \n",
    "if B+D==0:\n",
    "    print(\"Precision(Useless): undefined\")\n",
    "else:\n",
    "    print(\"Precision(Useless):\"+str(D/(B+D)))\n",
    "    \n",
    "print(\"Recall(Useless):\"+str(D/(C+D)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
