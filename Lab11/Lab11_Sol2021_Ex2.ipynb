{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFolder = \"inLab11_Ex2/\"\n",
    "prefixOutputFolder = \"out_Lab11_Ex2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the checkpoint folder (it is needed by some window transformations)\n",
    "ssc.checkpoint(\"checkpointfolder\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to the input folder\n",
    "tweetsDStream = ssc.textFileStream(inputFolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the content of each input line and return only the hashtags occurring in the \n",
    "# text of the tweet.\n",
    "# One pair (hashtag, +1) for each occurence of a hashtag in the analyzed line\n",
    "def extractHashtags(line):\n",
    "    #userId\\ttext_of_the_tweet\n",
    "    textTweet = line.split(\"\\t\")[1]\n",
    "    \n",
    "    words = textTweet.split(\" \")\n",
    "    \n",
    "    # Hashtags\n",
    "    hashtags = list(filter(lambda word: word.startswith(\"#\"), words))\n",
    "    \n",
    "    pairsHashtagOne = []\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "        pairsHashtagOne.append( (hashtag, 1) )\n",
    "        \n",
    "    return pairsHashtagOne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtagOneDStream = tweetsDStream.flatMap(extractHashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of occurrences of each (extracted) hashtag by considering only the\n",
    "# last 30 seconds of data (i.e., the last 30 seconds of data of the input data stream)\n",
    "# windowDuration = 30s\n",
    "# slideDuration = 10s\n",
    "hashtagNumOccurrencesDStream = hashtagOneDStream\\\n",
    ".reduceByKeyAndWindow(lambda v1, v2: v1+v2, lambda vnow, vold: vnow-vold, 30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the hashtags that occurred at least 100 times in the last 30 seconds\n",
    "selectedHashtagsDStream = hashtagNumOccurrencesDStream.filter(lambda pair: pair[1]>=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort hashtags by number of occurrences\n",
    "# You must use transform because sort is not available for DStreams\n",
    "sortedHashtagsDStream = selectedHashtagsDStream\\\n",
    ".transform(lambda batchRDD: batchRDD.sortBy(lambda pair: -1*pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in the output HDFS folders, sorted by number of occurrences, the pairs\n",
    "# (number of occurrences , hashtag) related to the last 30 seconds of data\n",
    "sortedHashtagsDStream.saveAsTextFiles(prefixOutputFolder, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print on the standard output the first 10 hashtags in terms of number of occurrences,\n",
    "# related to the last 30 seconds of data\n",
    "sortedHashtagsDStream.pprint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
